---
description: google-adk tutorial on BaseLlm, the abstraction for LLM backends, and LLMRegistry for model resolution.
globs: src/google/adk/models/base_llm.py
alwaysApply: false
---
# Chapter 6: BaseLlm

In the [previous chapter](event.mdc), we learned how `Event` objects capture the history of interactions within a session. Many of these events, particularly agent responses and decisions to use tools, originate from interactions with a Large Language Model (LLM). This chapter introduces `BaseLlm`, the core abstraction in `google-adk` that provides a standardized way to interact with various LLM backends.

## Motivation and Use Case

Different LLMs (like Google's Gemini, Anthropic's Claude, OpenAI's GPT, etc.) have distinct APIs, request/response formats, and capabilities (e.g., standard generation vs. live streaming). If agent logic were tightly coupled to a specific LLM's API, switching models would require significant code changes.

`BaseLlm` solves this by defining a common interface for interacting with any LLM backend. Concrete implementations like `Gemini`, `Claude`, or `LiteLlm` adapt the specific API calls of their respective services to this standard interface. This makes the framework, and agents built upon it, largely model-agnostic.

**Central Use Case:** An [Agent (BaseAgent / LlmAgent)](agent__baseagent___llmagent_.mdc) is configured with `model="gemini-1.5-flash-001"`. Internally, when the agent needs to generate a response or decide on an action, it uses the `BaseLlm` interface. The `LLMRegistry` resolves the string `"gemini-1.5-flash-001"` to a `Gemini` instance (a subclass of `BaseLlm`). The agent calls `generate_content_async` on this instance. If the developer later changes the configuration to `model="claude-3-opus-20240229"`, the `LLMRegistry` resolves it to a `Claude` instance, and the *same* agent code calling `generate_content_async` now interacts with the Claude backend, without needing modification (assuming credentials and dependencies are set up).

## Key Concepts

*   **`BaseLlm` Abstract Base Class (`base_llm.py`):**
    *   **Purpose:** Defines the standard contract for interacting with an LLM backend.
    *   **Core Interface:**
        *   `generate_content_async(llm_request: LlmRequest, stream: bool = False) -> AsyncGenerator[LlmResponse, None]`: The primary method for standard request/response generation (potentially streamed). Takes a standardized `LlmRequest` object and yields `LlmResponse` objects.
        *   `connect(llm_request: LlmRequest) -> BaseLlmConnection`: Establishes a persistent, potentially bidirectional connection for real-time interactions (e.g., live audio/video). Returns a `BaseLlmConnection` instance. (Less common than `generate_content_async`).
    *   **`model` Attribute:** Stores the specific model name (e.g., "gemini-1.5-flash-001").
    *   **`supported_models()` Class Method:** Returns a list of regex patterns matching the model names supported by a concrete implementation. Used by the `LLMRegistry`.

*   **Concrete Implementations (`google_llm.py`, `anthropic_llm.py`, `lite_llm.py`, etc.):**
    *   **Purpose:** Subclasses of `BaseLlm` that adapt a specific LLM service's API to the `BaseLlm` interface.
    *   **`Gemini`:** Interacts with Google's Gemini models via the `google-generativeai` Python SDK.
    *   **`Claude`:** Interacts with Anthropic's Claude models (specifically via Vertex AI in the provided snippet).
    *   **`LiteLlm`:** Uses the `litellm` library to provide a unified interface to a wide range of LLMs (OpenAI, Azure, Cohere, Anthropic, etc.), adapting their APIs to the `BaseLlm` standard. Requires setting appropriate environment variables for credentials.

*   **`LLMRegistry` (`registry.py`):**
    *   **Purpose:** A global registry that maps model name strings (which can include prefixes or patterns) to their corresponding `BaseLlm` implementation classes.
    *   **Mechanism:** Uses regex matching based on the patterns provided by `BaseLlm.supported_models()`.
    *   **`resolve(model: str) -> type[BaseLlm]`:** Finds the `BaseLlm` subclass responsible for the given model name string.
    *   **`new_llm(model: str) -> BaseLlm`:** Resolves the model name and returns an initialized instance of the corresponding `BaseLlm` subclass. This is the primary way `LlmAgent` gets its model instance.

*   **`BaseLlmConnection` (`base_llm_connection.py`):**
    *   **Purpose:** An abstraction for managing live, stateful, bidirectional connections to LLMs that support real-time interaction (e.g., Gemini's live content API). Defines methods like `send_history`, `send_content`, `send_realtime`, `receive`, and `close`. Returned by `BaseLlm.connect`.

*   **`LlmRequest` / `LlmResponse` (`llm_request.py`, `llm_response.py`):**
    *   **Purpose:** Standardized Pydantic models for structuring the data passed to and received from `BaseLlm.generate_content_async`. They encapsulate history (`contents`), configuration (`config`), tool definitions (`tools`), and response details (`content`, `finish_reason`, etc.) in a model-agnostic way, often mirroring `google.genai.types` structures.

## How to Use `BaseLlm`

Typically, you interact with `BaseLlm` implicitly through the [Agent (BaseAgent / LlmAgent)](agent__baseagent___llmagent_.mdc).

1.  **Implicitly via `LlmAgent` Configuration:**
    This is the most common way. Simply specify the desired model name string in the `LlmAgent` definition.

    ```python
    from google.adk.agents import LlmAgent

    # Agent configured to use Gemini Flash via the registry
    gemini_agent = LlmAgent(
        name="gemini_assistant",
        # LLMRegistry will resolve "gemini-1.5-flash-001" to the Gemini class
        model="gemini-1.5-flash-001",
        instruction="You are a helpful assistant.",
    )

    # Agent configured to use a Claude model via LiteLlm
    # Requires appropriate environment variables (e.g., ANTHROPIC_API_KEY)
    # and `pip install litellm`
    # from google.adk.models.lite_llm import LiteLlm # Need LiteLlm registered
    # LLMRegistry.register(LiteLlm) # Ensure LiteLlm is registered if not auto-done

    # Assuming LiteLlm is registered and supports this model string format
    claude_agent_via_litellm = LlmAgent(
        name="claude_assistant",
        # LLMRegistry *could* resolve this to LiteLlm if configured,
        # or potentially a dedicated Claude class.
        model="claude-3-haiku-20240307",
        instruction="You are a concise assistant.",
    )
    ```
    *Technical Explanation:* When `LlmAgent` initializes or needs its model (`canonical_model` property), it calls `LLMRegistry.new_llm(self.model)`. The registry uses the provided string ("gemini-1.5-flash-001" or "claude-3-haiku-20240307") to find the matching registered `BaseLlm` subclass (e.g., `Gemini` or `LiteLlm`) and returns an instance of it. The agent then interacts with this instance via the `BaseLlm` interface.

2.  **Direct Instantiation (Less Common):**
    You can instantiate a specific `BaseLlm` implementation directly if needed, bypassing the registry.

    ```python
    from google.adk.models.google_llm import Gemini
    from google.adk.agents import LlmAgent

    # Explicitly create a Gemini instance
    gemini_model_instance = Gemini(model="gemini-1.5-pro-001")

    # Pass the instance directly to the agent
    explicit_agent = LlmAgent(
        name="pro_assistant",
        model=gemini_model_instance, # Pass instance, not string
        instruction="You are a powerful assistant.",
    )
    ```
    *Technical Explanation:* Creating an instance like `Gemini(model=...)` gives you a concrete `BaseLlm` object. Passing this instance directly to `LlmAgent`'s `model` parameter bypasses the `LLMRegistry` lookup.

3.  **Registering a Custom LLM:**
    If you create your own `BaseLlm` subclass for an unsupported model, you need to register it.

    ```python
    # Assume MyCustomLlm is your class inheriting from BaseLlm
    # from my_custom_llm import MyCustomLlm
    from google.adk.models.registry import LLMRegistry

    # Register your custom class
    # LLMRegistry.register(MyCustomLlm)

    # Now, LlmAgent can use model names matching MyCustomLlm.supported_models()
    # custom_agent = LlmAgent(model="my-custom-model-v1", ...)
    ```
    *Technical Explanation:* Calling `LLMRegistry.register(YourLlmClass)` adds the regex patterns from `YourLlmClass.supported_models()` to the registry, mapping them to `YourLlmClass`. This allows `LLMRegistry.resolve` and `LLMRegistry.new_llm` to find and instantiate your custom class when the corresponding model names are used.

## Internal Implementation

*   **Interface Definition (`src/google/adk/models/base_llm.py`):**
    Defines the abstract methods that concrete implementations must provide.

    ```python
    # Simplified from src/google/adk/models/base_llm.py
    from abc import abstractmethod
    from typing import AsyncGenerator, TYPE_CHECKING
    from pydantic import BaseModel
    from .base_llm_connection import BaseLlmConnection

    if TYPE_CHECKING:
        from .llm_request import LlmRequest
        from .llm_response import LlmResponse

    class BaseLlm(BaseModel):
        model: str

        @classmethod
        def supported_models(cls) -> list[str]:
            return [] # Concrete classes override this

        @abstractmethod
        async def generate_content_async(
            self, llm_request: LlmRequest, stream: bool = False
        ) -> AsyncGenerator[LlmResponse, None]:
            # Abstract method for standard generation
            raise NotImplementedError(...)
            yield # Required for AsyncGenerator typing

        def connect(self, llm_request: LlmRequest) -> BaseLlmConnection:
            # Optional method for live connection
            raise NotImplementedError(...)
    ```
    *Technical Explanation:* An abstract base class using `abc.abstractmethod` (implicitly via Pydantic/override) to enforce the implementation of `generate_content_async` in subclasses. `connect` is also defined but not strictly required to be implemented if the backend doesn't support live connections.

*   **Registry (`src/google/adk/models/registry.py`):**
    Manages the mapping from model name patterns to classes.

    ```python
    # Simplified from src/google/adk/models/registry.py
    import re
    from functools import lru_cache
    from typing import TYPE_CHECKING

    if TYPE_CHECKING:
        from .base_llm import BaseLlm

    _llm_registry_dict: dict[str, type[BaseLlm]] = {} # Stores regex -> class

    class LLMRegistry:
        @staticmethod
        def register(llm_cls: type[BaseLlm]):
            # Called during import (e.g., in models/__init__.py)
            for regex in llm_cls.supported_models():
                _llm_registry_dict[regex] = llm_cls

        @staticmethod
        @lru_cache(maxsize=32) # Cache results for performance
        def resolve(model: str) -> type[BaseLlm]:
            for regex, llm_class in _llm_registry_dict.items():
                if re.compile(regex).fullmatch(model):
                    return llm_class
            raise ValueError(f'Model {model} not found.')

        @staticmethod
        def new_llm(model: str) -> BaseLlm:
            # Resolves the class and instantiates it
            llm_class = LLMRegistry.resolve(model)
            return llm_class(model=model)
    ```
    *Technical Explanation:* `register` populates `_llm_registry_dict` using patterns from `supported_models`. `resolve` iterates through the dictionary, performing regex matching to find the correct class. `new_llm` combines resolving and instantiation.

*   **Concrete Implementation (`src/google/adk/models/google_llm.py`):**
    The `Gemini` class implements `generate_content_async` using the `google-generativeai` library.

    ```python
    # Simplified from src/google/adk/models/google_llm.py
    from google.genai import Client, types
    from .base_llm import BaseLlm
    from .llm_request import LlmRequest
    from .llm_response import LlmResponse

    class Gemini(BaseLlm):
        model: str = 'gemini-1.5-flash'

        @staticmethod
        def supported_models() -> list[str]:
            return [r'gemini-.*', r'projects\/.+\/locations\/.+\/endpoints\/.+', ...]

        @property # Using cached_property internally
        def api_client(self) -> Client:
            # Creates and returns google.genai.Client instance
            # ... implementation detail ...
            return Client(...)

        async def generate_content_async(
            self, llm_request: LlmRequest, stream: bool = False
        ) -> AsyncGenerator[LlmResponse, None]:
            # Prepare request args (history, config) from llm_request
            # ... transformation logic ...

            if stream:
                responses = await self.api_client.aio.models.generate_content_stream(...)
                async for response in responses:
                    # Convert google response to LlmResponse
                    yield LlmResponse.create(response) # Handles partials
            else:
                response = await self.api_client.aio.models.generate_content(...)
                # Convert google response to LlmResponse
                yield LlmResponse.create(response)
    ```
    *Technical Explanation:* `Gemini` provides Gemini-specific model patterns. Its `generate_content_async` adapts the standard `LlmRequest` into the format expected by `google.genai.Client.generate_content` (or its streaming equivalent) and converts the `google.genai` response back into the standard `LlmResponse`.

*   **Concrete Implementation (`src/google/adk/models/lite_llm.py`):**
    The `LiteLlm` class implements `generate_content_async` using the `litellm` library.

    ```python
    # Simplified from src/google/adk/models/lite_llm.py
    from litellm import acompletion, completion, ModelResponse, Message
    from .base_llm import BaseLlm
    from .llm_request import LlmRequest
    from .llm_response import LlmResponse

    class LiteLlm(BaseLlm):
        # LiteLlm doesn't pre-define models, it relies on litellm's support
        @staticmethod
        def supported_models() -> list[str]:
            return [] # Or maybe a very generic pattern if desired

        async def generate_content_async(
            self, llm_request: LlmRequest, stream: bool = False
        ) -> AsyncGenerator[LlmResponse, None]:
            # Convert LlmRequest (history, tools, system prompt) to LiteLLM message format
            messages, tools = _get_completion_inputs(llm_request)
            # Prepare additional args if any

            if stream:
                # Use litellm.completion(..., stream=True)
                litellm_stream = completion(model=self.model, messages=messages, tools=tools, stream=True, ...)
                for chunk in litellm_stream:
                    # Convert LiteLLM chunk to LlmResponse (partial)
                    yield _model_response_to_generate_content_response(chunk, is_partial=True)
                # Potentially yield final aggregated response
            else:
                # Use litellm.acompletion(...)
                response: ModelResponse = await acompletion(model=self.model, messages=messages, tools=tools, ...)
                # Convert final LiteLLM response to LlmResponse
                yield _model_response_to_generate_content_response(response)
    ```
    *Technical Explanation:* `LiteLlm` acts as an adapter to the `litellm` library. It converts the standard `LlmRequest` into the message and tool format expected by `litellm.acompletion` or `litellm.completion`. It then converts the `litellm` response (or streaming chunks) back into the standard `LlmResponse`. It relies on `litellm` itself to handle the specific API calls to the underlying model specified by `self.model` (e.g., "openai/gpt-4", "anthropic/claude-3-haiku", etc.).

*   **Sequence Diagram (LlmAgent using BaseLlm):**

    ```mermaid
    sequenceDiagram
        participant Agent as LlmAgent
        participant Registry as LLMRegistry
        participant LLM as Resolved BaseLlm (e.g., Gemini)
        participant Backend as Actual LLM API (e.g., Google AI)

        Agent->>Agent: Needs to generate content (model="gemini-1.5-flash")
        Agent->>+Registry: new_llm("gemini-1.5-flash")
        Registry->>Registry: resolve("gemini-1.5-flash") matches Gemini pattern
        Registry-->>-Agent: gemini_instance (Gemini class)
        Agent->>+LLM: generate_content_async(llm_request, stream=True)
        LLM->>LLM: Convert LlmRequest to Backend format
        LLM->>+Backend: Call stream generation API
        loop Streaming Response Chunks
            Backend-->>-LLM: Raw chunk
            LLM->>LLM: Convert raw chunk to LlmResponse (partial)
            LLM-->>-Agent: yield llm_response (partial=True)
        end
        Backend-->>-LLM: Final chunk / End of stream
        LLM->>LLM: Convert final info to LlmResponse (partial=False)
        LLM-->>-Agent: yield llm_response (partial=False)
    ```
    *Technical Explanation:* The diagram shows an `LlmAgent` using the `LLMRegistry` to get a concrete `BaseLlm` instance based on its configured model string. The agent then calls `generate_content_async` on that instance. The instance (e.g., `Gemini`) translates the request/response between the standard ADK format (`LlmRequest`/`LlmResponse`) and the specific backend API format.

## Conclusion

`BaseLlm` and `LLMRegistry` are key components for achieving flexibility and model agnosticism in `google-adk`. `BaseLlm` defines a standard interface for LLM interaction, while concrete implementations handle the specifics of different backend APIs. The `LLMRegistry` allows developers to easily switch between models by simply changing a configuration string in their [Agent (BaseAgent / LlmAgent)](agent__baseagent___llmagent_.mdc), promoting code reusability and adaptability to the rapidly evolving LLM landscape.

With the core interaction mechanism defined, the next logical step is to understand how agents can leverage external capabilities beyond the LLM's built-in knowledge. The next chapter explores the [Tool (BaseTool)](tool__basetool_.mdc) abstraction, which allows LLMs to invoke functions and APIs.


---

Generated by [Rules for AI](https://github.com/altaidevorg/rules-for-ai)