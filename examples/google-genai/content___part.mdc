---
description: Explains google-genai's Content and Part structures, the core data format for conversational turns and multi-modal data exchange with models.
globs: 
alwaysApply: false
---
# Chapter 2: Content / Part

In [Chapter 1: Client](client.mdc), we learned how to configure the `Client` to connect to Google's Generative AI APIs. Now that we have a connection, we need to understand *how* to structure the data we send and receive. This chapter introduces the fundamental data structures for communication: `Content` and `Part`.

## Motivation and Use Case

Generative models, especially multi-modal ones like Gemini, process information in structured turns. A conversation isn't just a flat string; it involves roles (who is speaking?), different types of data (text, images, function calls/results), and potentially multiple pieces of data within a single turn.

The `Content` and `Part` objects provide this necessary structure. `Content` represents a single message or turn in the conversation, while `Part` represents individual pieces of data within that turn. This allows for rich interactions beyond simple text prompts.

**Central Use Case:** Imagine you want to ask a Gemini model to describe an image you provide. You can't just pass a raw image; you need to structure the request clearly indicating the text prompt ("What is in this image?") and the image data itself, both belonging to the 'user' role in a single turn.

```python
# Assuming 'client' is configured as shown in Chapter 1
from google.genai import types
import PIL.Image

# Load an image (replace with your image loading logic)
try:
    img = PIL.Image.open('path/to/your/image.jpg')
except FileNotFoundError:
    print("Please replace 'path/to/your/image.jpg' with an actual image file.")
    exit() # Or handle appropriately

# Explicitly construct Content and Part objects
image_part = types.Part.from_image(img) # Helper to create image part
text_part = types.Part.from_text("Describe this image.")

# Create a Content object for the user's turn
user_content = types.Content(
    role='user',
    parts=[text_part, image_part] # Multi-modal input
)

# Send the structured content to the model
response = client.models.generate_content(
    model='gemini-1.5-flash', # Use an appropriate multi-modal model
    contents=[user_content] # Pass a list of Content objects
)

print(response.text)
```
This example shows how `Content` (with `role='user'`) groups multiple `Part` objects (`text_part`, `image_part`) to form a single, multi-modal message turn.

## Key Concepts

Let's break down these structures:

### `Content`

*   **Purpose:** Represents a single, complete message or turn in a conversation history. It's the fundamental unit exchanged with the model via methods like `generate_content` or `send_message`.
*   **Structure:** Defined in `google.genai.types.Content`. It primarily contains:
    *   `role` (str): Identifies the author of the content. Common roles are:
        *   `'user'`: Content provided by the end-user interacting with the model.
        *   `'model'`: Content generated by the generative model.
        *   `'function'` (Deprecated, use 'tool'): Content representing the result of a function call requested by the model.
        *   `'tool'`: Content representing the result of executing a tool/function call requested by the model.
    *   `parts` (list[Part]): A list containing one or more `Part` objects that make up the content of this turn.
*   **Subclasses:** For convenience, `types.UserContent` (hardcoded `role='user'`) and `types.ModelContent` (hardcoded `role='model'`) exist.

```python
from google.genai import types

# A simple user text message
user_text_content = types.UserContent(parts=[types.Part.from_text("Hello!")])
# Equivalent to:
# user_text_content = types.Content(role='user', parts=[types.Part.from_text("Hello!")])

# A model's response containing text
model_text_content = types.ModelContent(parts=[types.Part.from_text("Hi there!")])
# Equivalent to:
# model_text_content = types.Content(role='model', parts=[types.Part.from_text("Hi there!")])

print(f"User Role: {user_text_content.role}")
print(f"Model Role: {model_text_content.role}")
```

### `Part`

*   **Purpose:** Represents a distinct piece of data within a `Content` object. A `Content` can contain multiple `Part`s, enabling multi-modal inputs or complex model outputs.
*   **Structure:** Defined in `google.genai.types.Part`. It's essentially a union type â€“ only *one* of its data fields should be populated:
    *   `text` (str): Plain text content.
    *   `inline_data` (Blob): Binary data (like an image or audio) directly embedded in the request. Often created from `PIL.Image` or bytes using helpers. Requires `mime_type` and `data` (bytes).
    *   `file_data` (FileData): A reference to a file previously uploaded via the [File API](api_modules__models__chats__files__tunings__caches__batches__operations__live_.mdc) (Gemini API only) or accessible via a URI (like a GCS path for Vertex AI). Requires `mime_type` and `file_uri`.
    *   `function_call` (FunctionCall): A request from the model to call a specific function/tool with given arguments. Used in [Function Calling Utilities](function_calling_utilities.mdc).
    *   `function_response` (FunctionResponse): The result provided by the user/developer after executing a requested `function_call`. Sent with `role='tool'`.
*   **Creation:** Use the `Part.from_...` class methods for common types or instantiate directly.

```python
from google.genai import types
import PIL.Image

# Text Part
text_part = types.Part.from_text("Analyze this data.")
print(f"Text Part: {text_part.text}")

# Inline Image Part (using a dummy image for example)
try:
    # Replace 'path/to/your/image.jpg' with your image file
    img = PIL.Image.open('path/to/your/image.jpg')
    inline_image_part = types.Part.from_image(img) # Uses PIL integration
    print(f"Inline Image Part MimeType: {inline_image_part.inline_data.mime_type}")
except FileNotFoundError:
     print("Skipping inline image part example: file not found.")
     inline_image_part = None
except ImportError:
     print("Skipping inline image part example: PIL/Pillow not installed.")
     inline_image_part = None

# File Data Part (referencing an uploaded file or GCS URI)
# Example using a GCS URI (Vertex AI)
file_data_part_gcs = types.Part.from_uri(
    file_uri='gs://cloud-samples-data/generative-ai/image/scones.jpg',
    mime_type='image/jpeg'
)
print(f"File Data Part URI: {file_data_part_gcs.file_data.file_uri}")

# Function Call Part (typically received from the model)
func_call_part = types.Part(
    function_call=types.FunctionCall(name='get_weather', args={'location': 'London'})
)
print(f"Function Call Part Name: {func_call_part.function_call.name}")

# Function Response Part (typically sent by the user/tool)
func_response_part = types.Part(
    function_response=types.FunctionResponse(name='get_weather', response={'weather': 'sunny'})
)
print(f"Function Response Part Name: {func_response_part.function_response.name}")

```

### Multi-modality

The power comes from combining `Part`s within a `Content`.

```python
from google.genai import types
import PIL.Image

# Assuming img is a loaded PIL.Image object as above
try:
    # Replace 'path/to/your/image.jpg' with your image file
    img = PIL.Image.open('path/to/your/image.jpg')
    multi_modal_content = types.UserContent(parts=[
        types.Part.from_text("What color is the main object in this image?"),
        types.Part.from_image(img) # Combine text and image
    ])
    print(f"Multi-modal Content: {len(multi_modal_content.parts)} parts")
except (FileNotFoundError, ImportError):
    print("Skipping multi-modal example.")

```

## Role of Transformers (`t_content`, `t_part`)

While you *can* manually construct `Content` and `Part` objects as shown above, it can be verbose for simple cases. The SDK provides [Transformers (`t_` functions)](transformers___t___functions_.mdc), particularly `t_content` and `t_part`, which automatically convert simpler Python types into the required `Content`/`Part` structures.

Many SDK methods, like `client.models.generate_content` and `chat.send_message`, use these transformers internally on their `contents` or `message` arguments.

```python
# Example from README.md - how a simple string is transformed
# Input to generate_content:
contents_input = 'Why is the sky blue?'

# Implicitly transformed by t_contents (inside generate_content) to:
# contents_output = [
#   types.UserContent(
#     parts=[
#       types.Part.from_text(text='Why is the sky blue?')
#     ]
#   )
# ]
# response = client.models.generate_content(model='gemini-1.5-flash', contents=contents_input)
```

```python
# Example from README.md - how a list of parts is transformed
# Input to generate_content:
contents_input = [
  types.Part.from_text('What is this image about?'),
  types.Part.from_uri(
    file_uri='gs://generativeai-downloads/images/scones.jpg',
    mime_type='image/jpeg',
  )
]

# Implicitly transformed by t_contents into a single UserContent:
# contents_output = [
#   types.UserContent(
#     parts=[
#       types.Part.from_text('What is this image about?'),
#       types.Part.from_uri(
#         file_uri='gs://generativeai-downloads/images/scones.jpg',
#         mime_type='image/jpeg',
#       )
#     ]
#   )
# ]
# response = client.models.generate_content(model='gemini-1.5-flash', contents=contents_input)
```

This abstraction simplifies common use cases, but understanding the underlying `Content`/`Part` structure is crucial for advanced scenarios like multi-modal input, function calling, and precise control over conversation history. You can find the transformation logic primarily in `google/genai/_transformers.py` within the `t_part` and `t_contents` functions.

## Internal Implementation

`Content` and `Part` are implemented as Pydantic models within `google/genai/types.py`. Pydantic provides data validation and serialization capabilities.

```python
# Simplified Pydantic-like structure (from google/genai/types.py)

class Blob(BaseModel): # Inherits from _common.BaseModel
    mime_type: str
    data: bytes # Base64 encoded/decoded automatically

class FileData(BaseModel):
    mime_type: str
    file_uri: str

class FunctionCall(BaseModel):
    name: str
    args: Optional[dict[str, Any]] = None
    id: Optional[str] = None # Gemini only

class FunctionResponse(BaseModel):
    name: str
    response: dict[str, Any]
    id: Optional[str] = None # Gemini only

class Part(BaseModel):
    text: Optional[str] = None
    inline_data: Optional[Blob] = None
    file_data: Optional[FileData] = None
    function_call: Optional[FunctionCall] = None
    function_response: Optional[FunctionResponse] = None
    # ... other potential fields (video, executable_code etc.)

    # Plus helper methods like Part.from_text, Part.from_image, ...

class Content(BaseModel):
    parts: list[Part]
    role: str
```
This Pydantic structure allows the SDK to easily validate input data and serialize/deserialize these objects when communicating with the backend API. The actual conversion to/from the specific API protocol buffers (Protobuf) happens deeper within the SDK, often involving converters like those found in `google/genai/_live_converters.py` used by the [BaseApiClient](baseapiclient.mdc).

```mermaid
graph LR
    A[User Input (str, PIL.Image, dict)] -- t_part / t_content --> B(types.Part / types.Content Pydantic Models);
    B -- model_dump / converters --> C(API Request JSON/Proto);
    D[API Response JSON/Proto] -- model_validate / converters --> E(types.Part / types.Content Pydantic Models);
    E --> F[SDK Response (e.g., response.text, response.parts)];
```

## Relationship to API Methods

`Content` and `Part` are central to the core generative methods:

*   **Input:** The `contents` parameter in `client.models.generate_content` and the `message` parameter in `chat.send_message` (covered in [Chat / AsyncChat](chat___asyncchat.mdc)) expect data that can be transformed into a `list[types.Content]`.
*   **Output:** The `GenerateContentResponse` object returned by these methods (and others like embedding methods) contains generated content within `response.candidates[...].content`, which is a `types.Content` object. The `response.text` and `response.parts` properties are convenient accessors derived from this underlying `Content` structure.

Understanding `Content` and `Part` is therefore essential for preparing complex inputs and correctly interpreting the structured outputs from the [API Modules (Models, Chats, Files, Tunings, Caches, Batches, Operations, Live)](api_modules__models__chats__files__tunings__caches__batches__operations__live_.mdc).

## Conclusion

`Content` and `Part` are the building blocks for structuring conversations and data exchange with Google's generative models. `Content` represents a turn with a specific `role`, and `Part` holds the individual data pieces (text, images, function calls/responses) within that turn. While helper transformers often simplify their creation, understanding their explicit structure is key to leveraging the full capabilities of the SDK, especially for multi-modal interactions and function calling.

With the client configured and the data structures understood, we can now explore the specific API functionalities offered by the SDK's modules in the next chapter: [API Modules (Models, Chats, Files, Tunings, Caches, Batches, Operations, Live)](api_modules__models__chats__files__tunings__caches__batches__operations__live_.mdc).


---

Generated by [Rules for AI](https://github.com/altaidevorg/rules-for-ai)